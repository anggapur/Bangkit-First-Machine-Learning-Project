{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Created at : \n",
    "28/03/2020\n",
    "##### Created by : \n",
    "Angga Pur, Henrico Aldy Ferdian, & Juli Andika\n",
    "##### Description : \n",
    "Process from get data, splitting data, feature scaling , training , evaluate, and logging\n",
    "You can choose to using 1.A or 1.B\n",
    "1.A => NOT convert numerical feature to categorical feature, creating dataset wiith dimension 400 x 152\n",
    "1.B => convert  numerical feature to categorical feature, creating dataset wiith dimension 400 x 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.layers import Input, Dense, Activation,Dropout\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Uses Tensorflow 2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A function to read data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(csv_url,columns_name,header=0):\n",
    "  cols = columns_name\n",
    "  data = pd.read_csv(r''+csv_url, names=cols, header=header).iloc[:, 1:]\n",
    "  return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A function to create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data,labels):\n",
    "  X = pd.concat(data, axis=1)\n",
    "  y = labels.values\n",
    "  return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Open the csv  and print the first 5 row of the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gender  age  estimated_salary  output\n",
      "0    Male   19             19000       0\n",
      "1    Male   35             20000       0\n",
      "2  Female   26             43000       0\n",
      "3  Female   27             57000       0\n",
      "4    Male   19             76000       0\n"
     ]
    }
   ],
   "source": [
    "columns = ['user_id','gender','age','estimated_salary','output']\n",
    "data = extract_data('dataset/Social_Network_Ads.csv',columns)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make numeric data to be categorical data\n",
    "the range is (a,b,c) => a is bottom_value, b is top_value+1, c is the step\n",
    "example : range(18,61,6) => bottom age is 18, toppest age is 60, the step is 5, so it will make 6 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"age\"] = pd.cut(data[\"age\"],range(18,61,5),include_lowest=True) # will be 6 class\n",
    "data[\"estimated_salary\"] = pd.cut(data[\"estimated_salary\"],range(15000,150001,22500),include_lowest=True) # will be 6 class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender = pd.get_dummies(data.gender,prefix='gender')\n",
    "age = pd.get_dummies(data.age,prefix='age')\n",
    "estimated_salary = pd.get_dummies(data.estimated_salary,prefix='estimated_salary')\n",
    "labels = pd.get_dummies(data.output,prefix='condition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = create_dataset([gender, age, estimated_salary],labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=np.random) #0 = not random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare layers\n",
    "input layer : adjust based on how many feature the dataset have\n",
    "hidden layer (1) : 100 node\n",
    "hidden layer (2) : 200 node\n",
    "hidden layer (3) : 200 node\n",
    "hidden layer (4) : 200 node\n",
    "hidden layer (5) : 200 node\n",
    "hidden layer (6) : 200 node\n",
    "hidden layer (7) : 200 node\n",
    "hidden layer (8) : 200 node\n",
    "hidden layer (9) : 200 node\n",
    "hidden layer (10) : 100 node\n",
    "output layer : adjust based on how many label the dataset have  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(X.shape[1],))\n",
    "dense_layer_1 = Dense(100, activation='relu')(input_layer)\n",
    "dense_layer_2 = Dense(200, activation='relu')(dense_layer_1)\n",
    "dense_layer_3 = Dense(200, activation='relu')(dense_layer_2)\n",
    "dense_layer_4 = Dense(200, activation='relu')(dense_layer_3)\n",
    "dense_layer_5 = Dense(200, activation='relu')(dense_layer_4)\n",
    "dense_layer_6 = Dense(200, activation='relu')(dense_layer_5)\n",
    "dense_layer_7 = Dense(200, activation='relu')(dense_layer_6)\n",
    "dense_layer_8 = Dense(200, activation='relu')(dense_layer_7)\n",
    "dense_layer_9 = Dense(200, activation='relu')(dense_layer_8)\n",
    "dense_layer_10 = Dense(100, activation='relu')(dense_layer_9)\n",
    "output = Dense(y.shape[1], activation='softmax')(dense_layer_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make model\n",
    "We use categorical crossentropy and adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 16)]              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               1700      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 323,602\n",
      "Trainable params: 323,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=input_layer, outputs=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare tensorboard log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir= os.path.join('logs','fit',datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),'')\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train model\n",
    "batch size 10 and 50 epoch. 80% train data and 20% validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256 samples, validate on 64 samples\n",
      "Epoch 1/50\n",
      "256/256 [==============================] - 1s 4ms/sample - loss: 0.5421 - acc: 0.6719 - val_loss: 0.4306 - val_acc: 0.8750\n",
      "Epoch 2/50\n",
      "256/256 [==============================] - 0s 861us/sample - loss: 0.4048 - acc: 0.8477 - val_loss: 0.4531 - val_acc: 0.8438\n",
      "Epoch 3/50\n",
      "256/256 [==============================] - 0s 822us/sample - loss: 0.2578 - acc: 0.9102 - val_loss: 0.3786 - val_acc: 0.8594\n",
      "Epoch 4/50\n",
      "256/256 [==============================] - 0s 866us/sample - loss: 0.2231 - acc: 0.9141 - val_loss: 0.3340 - val_acc: 0.8594\n",
      "Epoch 5/50\n",
      "256/256 [==============================] - 0s 990us/sample - loss: 0.1992 - acc: 0.9297 - val_loss: 0.4487 - val_acc: 0.8594\n",
      "Epoch 6/50\n",
      "256/256 [==============================] - 0s 996us/sample - loss: 0.1885 - acc: 0.9375 - val_loss: 0.4404 - val_acc: 0.8750\n",
      "Epoch 7/50\n",
      "256/256 [==============================] - 0s 949us/sample - loss: 0.1744 - acc: 0.9297 - val_loss: 0.5624 - val_acc: 0.8594\n",
      "Epoch 8/50\n",
      "256/256 [==============================] - 0s 907us/sample - loss: 0.1617 - acc: 0.9414 - val_loss: 0.8066 - val_acc: 0.8750\n",
      "Epoch 9/50\n",
      "256/256 [==============================] - 0s 908us/sample - loss: 0.1718 - acc: 0.9375 - val_loss: 0.5457 - val_acc: 0.8906\n",
      "Epoch 10/50\n",
      "256/256 [==============================] - 0s 822us/sample - loss: 0.1746 - acc: 0.9336 - val_loss: 0.4929 - val_acc: 0.8594\n",
      "Epoch 11/50\n",
      "256/256 [==============================] - 0s 822us/sample - loss: 0.1611 - acc: 0.9375 - val_loss: 1.1672 - val_acc: 0.8594\n",
      "Epoch 12/50\n",
      "256/256 [==============================] - 0s 824us/sample - loss: 0.1626 - acc: 0.9414 - val_loss: 0.8606 - val_acc: 0.8750\n",
      "Epoch 13/50\n",
      "256/256 [==============================] - 0s 820us/sample - loss: 0.1500 - acc: 0.9297 - val_loss: 1.1497 - val_acc: 0.8594\n",
      "Epoch 14/50\n",
      "256/256 [==============================] - 0s 774us/sample - loss: 0.1429 - acc: 0.9336 - val_loss: 1.0100 - val_acc: 0.8750\n",
      "Epoch 15/50\n",
      "256/256 [==============================] - 0s 782us/sample - loss: 0.1497 - acc: 0.9414 - val_loss: 1.2926 - val_acc: 0.8750\n",
      "Epoch 16/50\n",
      "256/256 [==============================] - ETA: 0s - loss: 0.1630 - acc: 0.933 - 0s 778us/sample - loss: 0.1425 - acc: 0.9453 - val_loss: 1.3132 - val_acc: 0.8594\n",
      "Epoch 17/50\n",
      "256/256 [==============================] - 0s 777us/sample - loss: 0.1351 - acc: 0.9453 - val_loss: 1.3191 - val_acc: 0.8594\n",
      "Epoch 18/50\n",
      "256/256 [==============================] - 0s 736us/sample - loss: 0.1276 - acc: 0.9492 - val_loss: 1.3869 - val_acc: 0.8594\n",
      "Epoch 19/50\n",
      "256/256 [==============================] - 0s 735us/sample - loss: 0.1347 - acc: 0.9453 - val_loss: 1.5446 - val_acc: 0.8594\n",
      "Epoch 20/50\n",
      "256/256 [==============================] - 0s 910us/sample - loss: 0.1324 - acc: 0.9453 - val_loss: 1.5250 - val_acc: 0.8594\n",
      "Epoch 21/50\n",
      "256/256 [==============================] - 0s 908us/sample - loss: 0.1262 - acc: 0.9492 - val_loss: 1.6204 - val_acc: 0.8594\n",
      "Epoch 22/50\n",
      "256/256 [==============================] - 0s 907us/sample - loss: 0.1299 - acc: 0.9492 - val_loss: 1.5752 - val_acc: 0.8594\n",
      "Epoch 23/50\n",
      "256/256 [==============================] - 0s 910us/sample - loss: 0.1360 - acc: 0.9453 - val_loss: 1.4914 - val_acc: 0.8594\n",
      "Epoch 24/50\n",
      "256/256 [==============================] - 0s 864us/sample - loss: 0.1283 - acc: 0.9492 - val_loss: 1.5879 - val_acc: 0.8594\n",
      "Epoch 25/50\n",
      "256/256 [==============================] - 0s 820us/sample - loss: 0.1303 - acc: 0.9492 - val_loss: 1.4386 - val_acc: 0.8594\n",
      "Epoch 26/50\n",
      "256/256 [==============================] - 0s 952us/sample - loss: 0.1255 - acc: 0.9453 - val_loss: 1.6929 - val_acc: 0.8594\n",
      "Epoch 27/50\n",
      "256/256 [==============================] - 0s 775us/sample - loss: 0.1299 - acc: 0.9492 - val_loss: 1.6494 - val_acc: 0.8594\n",
      "Epoch 28/50\n",
      "256/256 [==============================] - 0s 735us/sample - loss: 0.1334 - acc: 0.9453 - val_loss: 1.5947 - val_acc: 0.8594\n",
      "Epoch 29/50\n",
      "256/256 [==============================] - 0s 1ms/sample - loss: 0.1333 - acc: 0.9492 - val_loss: 1.7832 - val_acc: 0.8594\n",
      "Epoch 30/50\n",
      "256/256 [==============================] - 0s 738us/sample - loss: 0.1227 - acc: 0.9492 - val_loss: 1.8742 - val_acc: 0.8594\n",
      "Epoch 31/50\n",
      "256/256 [==============================] - 0s 736us/sample - loss: 0.1305 - acc: 0.9492 - val_loss: 1.6996 - val_acc: 0.8594\n",
      "Epoch 32/50\n",
      "256/256 [==============================] - ETA: 0s - loss: 0.1440 - acc: 0.940 - 0s 732us/sample - loss: 0.1314 - acc: 0.9492 - val_loss: 1.3500 - val_acc: 0.8750\n",
      "Epoch 33/50\n",
      "256/256 [==============================] - 0s 737us/sample - loss: 0.1225 - acc: 0.9492 - val_loss: 2.1136 - val_acc: 0.8594\n",
      "Epoch 34/50\n",
      "256/256 [==============================] - 0s 777us/sample - loss: 0.1297 - acc: 0.9492 - val_loss: 2.1066 - val_acc: 0.8594\n",
      "Epoch 35/50\n",
      "256/256 [==============================] - 0s 863us/sample - loss: 0.1344 - acc: 0.9492 - val_loss: 1.8759 - val_acc: 0.8594\n",
      "Epoch 36/50\n",
      "256/256 [==============================] - 0s 908us/sample - loss: 0.1266 - acc: 0.9492 - val_loss: 2.0556 - val_acc: 0.8594\n",
      "Epoch 37/50\n",
      "256/256 [==============================] - 0s 910us/sample - loss: 0.1280 - acc: 0.9414 - val_loss: 1.8492 - val_acc: 0.8594\n",
      "Epoch 38/50\n",
      "256/256 [==============================] - 0s 864us/sample - loss: 0.1297 - acc: 0.9375 - val_loss: 1.7369 - val_acc: 0.8750\n",
      "Epoch 39/50\n",
      "256/256 [==============================] - 0s 818us/sample - loss: 0.1274 - acc: 0.9453 - val_loss: 1.3813 - val_acc: 0.8594\n",
      "Epoch 40/50\n",
      "256/256 [==============================] - 0s 822us/sample - loss: 0.1372 - acc: 0.9492 - val_loss: 1.5541 - val_acc: 0.8594\n",
      "Epoch 41/50\n",
      "256/256 [==============================] - 0s 823us/sample - loss: 0.1384 - acc: 0.9492 - val_loss: 1.8916 - val_acc: 0.8594\n",
      "Epoch 42/50\n",
      "256/256 [==============================] - 0s 821us/sample - loss: 0.1555 - acc: 0.9492 - val_loss: 1.2995 - val_acc: 0.8750\n",
      "Epoch 43/50\n",
      "256/256 [==============================] - 0s 777us/sample - loss: 0.3294 - acc: 0.9062 - val_loss: 0.5406 - val_acc: 0.8594\n",
      "Epoch 44/50\n",
      "256/256 [==============================] - 0s 732us/sample - loss: 0.3407 - acc: 0.8828 - val_loss: 0.3570 - val_acc: 0.8594\n",
      "Epoch 45/50\n",
      "256/256 [==============================] - 0s 734us/sample - loss: 0.2153 - acc: 0.9336 - val_loss: 2.4075 - val_acc: 0.8906\n",
      "Epoch 46/50\n",
      "256/256 [==============================] - ETA: 0s - loss: 0.2390 - acc: 0.942 - 0s 734us/sample - loss: 0.2519 - acc: 0.9219 - val_loss: 0.6316 - val_acc: 0.8750\n",
      "Epoch 47/50\n",
      "256/256 [==============================] - 0s 777us/sample - loss: 0.1751 - acc: 0.9453 - val_loss: 1.2365 - val_acc: 0.8750\n",
      "Epoch 48/50\n",
      "256/256 [==============================] - 0s 778us/sample - loss: 0.1779 - acc: 0.9375 - val_loss: 0.7115 - val_acc: 0.8906\n",
      "Epoch 49/50\n",
      "256/256 [==============================] - 0s 863us/sample - loss: 0.1628 - acc: 0.9297 - val_loss: 1.0598 - val_acc: 0.8750\n",
      "Epoch 50/50\n",
      "256/256 [==============================] - 0s 911us/sample - loss: 0.1658 - acc: 0.9414 - val_loss: 1.1115 - val_acc: 0.8750\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=10, epochs=50, verbose=1, validation_split=0.2, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('saved_model/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 74us/sample - loss: 0.8240 - acc: 0.8750\n",
      "Test Score: 0.8240363240242005\n",
      "Test Accuracy: 0.875\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Launch tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 17544), started 0:19:27 ago. (Use '!kill 17544' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e951f3f5f1e527e1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e951f3f5f1e527e1\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
